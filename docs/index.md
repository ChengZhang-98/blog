---
hide:
   - footer
   - navigation
   - toc
---


# About me

![selfie](./assets/images/selfie.jpg){ width=400px .left }

[:fontawesome-solid-envelope:](mailto: chengzhang98@outlook.com) [:fontawesome-brands-linkedin:](https://www.linkedin.com/in/chengzhang98) [:fontawesome-brands-github:](https://github.com/ChengZhang-98)

<p markdown>
I am Cheng Zhang, a PhD student in [Circuits and System Group](https://www.imperial.ac.uk/electrical-engineering/research/circuits-and-systems/), the Department of Electrical and Electronic Engineering, Imperial College London, supervised by [Dr Yiren (Aaron) Zhao](https://aaron-zhao123.github.io/) and [Prof George A. Constantinides](https://www.linkedin.com/in/george-constantinides-5b12098).
</p>

<p markdown>
My research is currently sponsored by the [Scaling Compute Program under ARIA](https://www.aria.org.uk/scaling-compute/), and previously by [SpatialML](https://spatialml.net/). My research interests mainly include efficient machine learning and AI acceleration. My CV can be found [here](./assets/pdfs/CV.pdf).
</p>

## Education

- PhD in Electrical and Electronic Engineering, Imperial College London, Jan 2023 - Current
- MSc in Electronics, The University of Edinburgh, Sep 2021 - Aug 2022
- BEng in Automation, Beihang University, Sep 2017 - Jun 2021

## Publications

> $^\dag$ denotes equal contribution

- [<u>\[ACL2025\]</u>](https://arxiv.org/abs/2412.13488) Xinxin Liu, Aaron Thomas, **Cheng Zhang**, Jianyi Cheng, Yiren Zhao, Xitong Gao. Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models. *The 63rd Annual Meeting of the Association for Computational Linguistics*.

- [<u>\[ICML2025\]</u>](https://arxiv.org/abs/2411.05197v1) **Cheng Zhang**$^\dag$, Hanna Foerster$^\dag$, Robert D. Mullins, Yiren Zhao, Ilia Shumailov. Hardware and Software Platform Inference. *The 42nd International Conference on Machine Learning*.

- [<u>\[ICLR2025\]</u>](https://arxiv.org/abs/2410.06040). **Cheng Zhang**, Jeffrey T. H. Wong, Can Xiao, George A. Constantinides, Yiren Zhao. QERA: an Analytical Framework for Quantization Error Reconstruction. *The 13th International Conference on Learning Representations*.

- [<u>\[ICML2024\]</u>](https://arxiv.org/abs/2402.02446) **Cheng Zhang**, Jianyi Cheng, George A. Constantinides, Yiren Zhao. LQER: Low-Rank Quantization Error Reconstruction for LLMs. *The Forty-first International Conference on Machine Learning*.

- [<u>\[EMNLP2023\]</u>](https://aclanthology.org/2023.emnlp-main.617.pdf) **Cheng Zhang**, Jianyi Cheng, Ilia Shumailov, George Anthony Constantinides, Yiren Zhao. Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference? *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*.

- [<u>\[IEEE SaTML2025\]</u>](https://arxiv.org/abs/2405.20990). Eleanor Clifford, Adhithya Saravanan, Harry Langford, **Cheng Zhang**, Yiren Zhao, Robert Mullins, Ilia Shumailov, Jamie Hayes. Locking Machine Learning Models into Hardware. *The 3rd IEEE Conference on Secure and Trustworthy Machine Learning*.

- [<u>\[ICML2024 Workshop\]</u>](https://arxiv.org/abs/2406.14963). Yuang Chen, **Cheng Zhang**, Xitong Gao, Robert D. Mullins, George A. Constantinides, Yiren Zhao. Optimised Grouped-Query Attention Mechanism for Transformers.

- [<u>\[ICML2024 Workshop\]</u>](https://arxiv.org/abs/2406.14956) Zixi Zhang, **Cheng Zhang**, Xitong Gao, Robert D. Mullins, George A. Constantinides, Yiren Zhao. Unlocking the Global Synergies in Low-Rank Adapters.



- [<u>[FPL2024]</u>](https://arxiv.org/abs/2406.03088). Zhewen Yu, Sudarshan Sreeram, Krish Agrawal, Junyi Wu, Alexander Montgomerie-Corcoran, **Cheng Zhang**, Jianyi Cheng, Christos-Savvas Bouganis, Yiren Zhao. HASS: Hardware-Aware Sparsity Search for Dataflow DNN Accelerator

- [<u>\[NeurIPS2023 Workshop\]</u>](http://mlforsystems.org/) **Cheng Zhang**, Jianyi Cheng, Zhewen Yu, Yiren Zhao. MASE: An Efficient Representation for Software-Defined ML Hardware System Exploration. *Workshop on ML for Systems at NeurIPS 2023*.


